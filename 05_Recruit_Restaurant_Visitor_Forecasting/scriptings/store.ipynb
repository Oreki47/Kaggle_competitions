{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, re\n",
    "import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "    'ar': pd.read_csv('../data/air_reserve.csv'),\n",
    "    'as': pd.read_csv('../data/air_store_info.csv'),\n",
    "    'hs': pd.read_csv('../data/hpg_store_info.csv'),\n",
    "    'trn': pd.read_csv('../data/air_visit_data.csv'),  # with the visitors column, which is the target\n",
    "    'hr': pd.read_csv('../data/hpg_reserve.csv'),\n",
    "    'id': pd.read_csv('../data/store_id_relation.csv'),\n",
    "    'tst': pd.read_csv('../data/sample_submission.csv'),\n",
    "    'hol': pd.read_csv('../data/date_info.csv').rename(columns={'calendar_date':'visit_date'})  # advanced features\n",
    "    }\n",
    "print(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = data['trn'].shape[0]\n",
    "test_size = data['tst'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "air_store dataframe shape: (829, 57)\n"
     ]
    }
   ],
   "source": [
    "data['tst']['air_store_id'] = data['tst']['id'].apply(lambda x: (\"_\").join(x.split('_')[:2]))\n",
    "data['tst']['visit_date'] = data['tst']['id'].apply(lambda x: x.split('_')[2])\n",
    "data['full'] = pd.concat([data['trn'], data['tst']])\n",
    "\n",
    "data['full']['visit_date'] = pd.to_datetime(data['full']['visit_date'])\n",
    "data['full']['dow'] = data['full']['visit_date'].dt.dayofweek\n",
    "data['full']['year'] = data['full']['visit_date'].dt.year\n",
    "data['full']['month'] = data['full']['visit_date'].dt.month\n",
    "data['full']['doy'] = data['full']['visit_date'].dt.dayofyear\n",
    "data['full']['dom'] = data['full']['visit_date'].dt.days_in_month\n",
    "data['full']['woy'] = data['full']['visit_date'].dt.weekofyear\n",
    "data['full']['is_month_end'] = data['full']['visit_date'].dt.is_month_end\n",
    "data['full']['visit_date'] = data['full']['visit_date'].dt.date\n",
    "data['full']['date_int'] = data['full']['visit_date'].apply(lambda x: x.strftime('%Y%m%d')).astype(int)\n",
    "data['full'].head()\n",
    "\n",
    "# reserve data handle\n",
    "data['ar']['visit_datetime'] = pd.to_datetime(data['ar']['visit_datetime'])\n",
    "data['ar']['reserve_datetime'] = pd.to_datetime(data['ar']['reserve_datetime'])\n",
    "data['ar']['visit_date'] = data['ar']['visit_datetime'].dt.date\n",
    "data['ar']['visit_time'] = data['ar']['visit_datetime'].dt.hour\n",
    "data['ar']['reserve_date'] = data['ar']['reserve_datetime'].dt.date\n",
    "data['ar']['reserve_time'] = data['ar']['reserve_datetime'].dt.hour\n",
    "data['ar']['visit_minus_reverse_hours'] = (data['ar']['visit_date'] - data['ar']['reserve_date']).dt.days * 24 + (data['ar']['visit_time'] - data['ar']['reserve_time'])\n",
    "data['ar']['visit_minus_reverse_days'] = (data['ar']['visit_date'] - data['ar']['reserve_date']).dt.days\n",
    "# for later merge operation\n",
    "data['ar']['reserve_date'] = data['ar']['reserve_date'].apply(lambda x: str(x))\n",
    "data['ar']['visit_date'] = data['ar']['visit_date'].apply(lambda x: str(x))\n",
    "\n",
    "# groupby operations under ar_g\n",
    "data['ar_g'] = data['ar'].groupby(['air_store_id', 'visit_date'])['reserve_visitors'].agg('sum').\\\n",
    "                   reset_index(drop=False).rename(columns={'reserve_visitors': 'reserve_ppl_count'})\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['reserve_visitors'].agg('count').\\\n",
    "                   reset_index(drop=False).rename(columns={'reserve_visitors': 'reserve_tot_count'}),\n",
    "    on=['air_store_id', 'visit_date'])\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('mean').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'avg_reserve_hr_day'}),\n",
    "    on=['air_store_id', 'visit_date'])\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('max').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'max_reserve_hr'}),\n",
    "    on=['air_store_id', 'visit_date'])\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('min').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'min_reserve_hr'}),\n",
    "    on=['air_store_id', 'visit_date'])\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('mean').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'mean_reserve_hr'}),\n",
    "    on=['air_store_id', 'visit_date'])\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['visit_minus_reverse_days'].agg('min').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_days': 'min_reserve_dy'}),\n",
    "    on=['air_store_id', 'visit_date'])\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['visit_minus_reverse_days'].agg('mean').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_days': 'mean_reserve_dy'}),\n",
    "    on=['air_store_id', 'visit_date'])\n",
    "\n",
    "data['ar_g'] = pd.merge(\n",
    "    data['ar_g'], \n",
    "    data['ar'].groupby(['air_store_id', 'visit_date'])['visit_minus_reverse_days'].agg('max').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_days': 'max_reserve_dy'}),\n",
    "    on=['air_store_id', 'visit_date'])  \n",
    "\n",
    "# hpg reservation handle\n",
    "data['hr']['visit_datetime'] = pd.to_datetime(data['hr']['visit_datetime'])\n",
    "data['hr']['reserve_datetime'] = pd.to_datetime(data['hr']['reserve_datetime'])\n",
    "data['hr']['visit_date'] = data['hr']['visit_datetime'].dt.date\n",
    "data['hr']['visit_time'] = data['hr']['visit_datetime'].dt.hour\n",
    "data['hr']['reserve_date'] = data['hr']['reserve_datetime'].dt.date\n",
    "data['hr']['reserve_time'] = data['hr']['reserve_datetime'].dt.hour\n",
    "data['hr']['visit_minus_reverse_hours'] = (data['hr']['visit_date'] - data['hr']['reserve_date']).dt.days * 24 + (data['hr']['visit_time'] - data['hr']['reserve_time'])\n",
    "data['hr']['visit_minus_reverse_days'] = (data['hr']['visit_date'] - data['hr']['reserve_date']).dt.days\n",
    "# for later merge operation\n",
    "data['hr']['reserve_date'] = data['hr']['reserve_date'].apply(lambda x: str(x))\n",
    "data['hr']['visit_date'] = data['hr']['visit_date'].apply(lambda x: str(x))\n",
    "\n",
    "# groupby operations under hr_g\n",
    "data['hr_g'] = data['hr'].groupby(['hpg_store_id', 'visit_date'])['reserve_visitors'].agg('sum').\\\n",
    "                   reset_index(drop=False).rename(columns={'reserve_visitors': 'reserve_ppl_count'})\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['reserve_visitors'].agg('count').\\\n",
    "                   reset_index(drop=False).rename(columns={'reserve_visitors': 'reserve_tot_count'}),\n",
    "    on=['hpg_store_id', 'visit_date'])\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('mean').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'avg_reserve_hr_day'}),\n",
    "    on=['hpg_store_id', 'visit_date'])\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('max').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'max_reserve_hr'}),\n",
    "    on=['hpg_store_id', 'visit_date'])\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('min').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'min_reserve_hr'}),\n",
    "    on=['hpg_store_id', 'visit_date'])\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['visit_minus_reverse_hours'].agg('mean').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_hours': 'mean_reserve_hr'}),\n",
    "    on=['hpg_store_id', 'visit_date'])\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['visit_minus_reverse_days'].agg('min').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_days': 'min_reserve_dy'}),\n",
    "    on=['hpg_store_id', 'visit_date'])\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['visit_minus_reverse_days'].agg('mean').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_days': 'mean_reserve_dy'}),\n",
    "    on=['hpg_store_id', 'visit_date'])\n",
    "\n",
    "data['hr_g'] = pd.merge(\n",
    "    data['hr_g'], \n",
    "    data['hr'].groupby(['hpg_store_id', 'visit_date'])['visit_minus_reverse_days'].agg('max').\\\n",
    "        reset_index(drop=False).rename(columns={'visit_minus_reverse_days': 'max_reserve_dy'}),\n",
    "    on=['hpg_store_id', 'visit_date']) \n",
    "\n",
    "data['hr_g'] = pd.merge(data['id'], data['hr_g'], on='hpg_store_id', how='left')\n",
    "data['ar_g'] = pd.merge(data['ar_g'], data['hr_g'], on=['air_store_id', 'visit_date'], how='left', suffixes=('_air', '_hpg'))\n",
    "\n",
    "# air_store handle\n",
    "data['as'].air_area_name = data['as'].air_area_name.apply(lambda x: re.sub(\" \\d+\", \" \", x))\n",
    "data['as']['air_lv1'] = data['as'].air_area_name.apply(lambda x: x.split(\" \")[0])\n",
    "data['as']['air_lv2'] = data['as'].air_area_name.apply(lambda x: x.split(\" \")[1])\n",
    "data['as']['air_lv3'] = data['as'].air_area_name.apply(lambda x: x.split(\" \")[2])\n",
    "data['as']['air_lv4'] = data['as'].air_lv2.apply(lambda x: x.split('-')[1])  # 市/郡/区\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby(['latitude','longitude']).air_store_id.count().reset_index().rename(columns={'air_store_id':'air_stores_on_same_addr'}),\n",
    "    how='left', on=['latitude','longitude'])\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv1').air_store_id.count().reset_index().rename(columns={'air_store_id':'air_stores_lv1'}),\n",
    "    how='left', on='air_lv1')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby(['air_lv1', 'air_lv2']).air_store_id.count().reset_index().rename(columns={'air_store_id':'air_stores_lv2'}),\n",
    "    how='left', on=['air_lv1', 'air_lv2'])\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby(['air_lv1', 'air_lv2', 'air_lv3']).air_store_id.count().reset_index().rename(columns={'air_store_id':'air_stores_lv3'}),\n",
    "    how='left', on=['air_lv1', 'air_lv2', 'air_lv3'])   \n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv1').latitude.mean().reset_index().rename(columns={'latitude':'mean_lat_air_lv1'}),\n",
    "    how='left', on='air_lv1')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv1').latitude.max().reset_index().rename(columns={'latitude':'max_lat_air_lv1'}),\n",
    "    how='left', on='air_lv1')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv1').latitude.min().reset_index().rename(columns={'latitude':'min_lat_air_lv1'}),\n",
    "    how='left', on='air_lv1')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv1').longitude.mean().reset_index().rename(columns={'longitude':'mean_lon_air_lv1'}),\n",
    "    how='left', on='air_lv1')   \n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv1').longitude.max().reset_index().rename(columns={'longitude':'max_lon_air_lv1'}),\n",
    "    how='left', on='air_lv1')   \n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv1').longitude.min().reset_index().rename(columns={'longitude':'min_lon_air_lv1'}),\n",
    "    how='left', on='air_lv1')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv2').latitude.mean().reset_index().rename(columns={'latitude':'mean_lat_air_lv2'}),\n",
    "    how='left', on='air_lv2')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv2').latitude.max().reset_index().rename(columns={'latitude':'max_lat_air_lv2'}),\n",
    "    how='left', on='air_lv2')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv2').latitude.min().reset_index().rename(columns={'latitude':'min_lat_air_lv2'}),\n",
    "    how='left', on='air_lv2')\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv2').longitude.mean().reset_index().rename(columns={'longitude':'mean_lon_air_lv2'}),\n",
    "    how='left', on='air_lv2')   \n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv2').longitude.max().reset_index().rename(columns={'longitude':'max_lon_air_lv2'}),\n",
    "    how='left', on='air_lv2')   \n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_lv2').longitude.min().reset_index().rename(columns={'longitude':'min_lon_air_lv2'}),\n",
    "    how='left', on='air_lv2')   \n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby('air_genre_name').air_store_id.count().reset_index().rename(columns={'air_store_id':'air_genre_count'}),\n",
    "    how='left', on=['air_genre_name'])\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby(['air_genre_name', 'air_lv1']).air_store_id.count().reset_index().rename(columns={'air_store_id':'air_genre_count_lv1'}),\n",
    "    how='left', on=['air_genre_name', 'air_lv1'])\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby(['air_genre_name', 'air_lv1', 'air_lv2']).air_store_id.count().reset_index().rename(columns={'air_store_id':'air_genre_count_lv2'}),\n",
    "    how='left', on=['air_genre_name', 'air_lv1', 'air_lv2'])\n",
    "\n",
    "data['as'] = pd.merge(\n",
    "    data['as'],\n",
    "    data['as'].groupby(['air_genre_name', 'air_lv1', 'air_lv2', 'air_lv3']).air_store_id.count().reset_index().rename(columns={'air_store_id':'air_genre_count_lv3'}),\n",
    "    how='left', on=['air_genre_name', 'air_lv1', 'air_lv2', 'air_lv3'])\n",
    "\n",
    "# hpg_store same handle without hpg_lv4\n",
    "data['hs'].hpg_area_name = data['hs'].hpg_area_name.apply(lambda x: re.sub(\" \\d+\", \" \", x))\n",
    "data['hs']['hpg_lv1'] = data['hs'].hpg_area_name.apply(lambda x: x.split(\" \")[0])\n",
    "data['hs']['hpg_lv2'] = data['hs'].hpg_area_name.apply(lambda x: x.split(\" \")[1])\n",
    "data['hs']['hpg_lv3'] = data['hs'].hpg_area_name.apply(lambda x: x.split(\" \")[2])\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby(['latitude','longitude']).hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_stores_on_same_addr'}),\n",
    "    how='left', on=['latitude','longitude'])\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv1').hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_stores_lv1'}),\n",
    "    how='left', on='hpg_lv1')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby(['hpg_lv1', 'hpg_lv2']).hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_stores_lv2'}),\n",
    "    how='left', on=['hpg_lv1', 'hpg_lv2'])\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby(['hpg_lv1', 'hpg_lv2', 'hpg_lv3']).hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_stores_lv3'}),\n",
    "    how='left', on=['hpg_lv1', 'hpg_lv2', 'hpg_lv3'])   \n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv1').latitude.mean().reset_index().rename(columns={'latitude':'mean_lat_hpg_lv1'}),\n",
    "    how='left', on='hpg_lv1')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv1').latitude.max().reset_index().rename(columns={'latitude':'max_lat_hpg_lv1'}),\n",
    "    how='left', on='hpg_lv1')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv1').latitude.min().reset_index().rename(columns={'latitude':'min_lat_hpg_lv1'}),\n",
    "    how='left', on='hpg_lv1')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv1').longitude.mean().reset_index().rename(columns={'longitude':'mean_lon_hpg_lv1'}),\n",
    "    how='left', on='hpg_lv1')   \n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv1').longitude.max().reset_index().rename(columns={'longitude':'max_lon_hpg_lv1'}),\n",
    "    how='left', on='hpg_lv1')   \n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv1').longitude.min().reset_index().rename(columns={'longitude':'min_lon_hpg_lv1'}),\n",
    "    how='left', on='hpg_lv1')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv2').latitude.mean().reset_index().rename(columns={'latitude':'mean_lat_hpg_lv2'}),\n",
    "    how='left', on='hpg_lv2')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv2').latitude.max().reset_index().rename(columns={'latitude':'max_lat_hpg_lv2'}),\n",
    "    how='left', on='hpg_lv2')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv2').latitude.min().reset_index().rename(columns={'latitude':'min_lat_hpg_lv2'}),\n",
    "    how='left', on='hpg_lv2')\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv2').longitude.mean().reset_index().rename(columns={'longitude':'mean_lon_hpg_lv2'}),\n",
    "    how='left', on='hpg_lv2')   \n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv2').longitude.max().reset_index().rename(columns={'longitude':'max_lon_hpg_lv2'}),\n",
    "    how='left', on='hpg_lv2')   \n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_lv2').longitude.min().reset_index().rename(columns={'longitude':'min_lon_hpg_lv2'}),\n",
    "    how='left', on='hpg_lv2')   \n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby('hpg_genre_name').hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_genre_count'}),\n",
    "    how='left', on=['hpg_genre_name'])\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby(['hpg_genre_name', 'hpg_lv1']).hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_genre_count_lv1'}),\n",
    "    how='left', on=['hpg_genre_name', 'hpg_lv1'])\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby(['hpg_genre_name', 'hpg_lv1', 'hpg_lv2']).hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_genre_count_lv2'}),\n",
    "    how='left', on=['hpg_genre_name', 'hpg_lv1', 'hpg_lv2'])\n",
    "\n",
    "data['hs'] = pd.merge(\n",
    "    data['hs'],\n",
    "    data['hs'].groupby(['hpg_genre_name', 'hpg_lv1', 'hpg_lv2', 'hpg_lv3']).hpg_store_id.count().reset_index().rename(columns={'hpg_store_id':'hpg_genre_count_lv3'}),\n",
    "    how='left', on=['hpg_genre_name', 'hpg_lv1', 'hpg_lv2', 'hpg_lv3'])\n",
    "\n",
    "# merge as and hs\n",
    "data['hs'] = pd.merge(data['id'], data['hs'], on='hpg_store_id', how='left')\n",
    "data['as'] = pd.merge(data['as'], data['hs'], on='air_store_id', how='left', suffixes=('_air', '_hpg'))\n",
    "print('air_store dataframe shape:', data['as'].shape)\n",
    "\n",
    "cat_vars = [\n",
    "    'air_genre_name', 'air_lv1', 'air_lv2', 'air_lv3', 'air_lv4',\n",
    "    'hpg_genre_name', 'hpg_lv1', 'hpg_lv2', 'hpg_lv3'\n",
    "]\n",
    "\n",
    "lb = LabelEncoder()\n",
    "for cat_var in cat_vars:\n",
    "    data['as'][cat_var] = lb.fit_transform(data['as'][cat_var].astype(str))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full'].visit_date = data['full'].visit_date.apply(lambda x: str(x))\n",
    "temp = pd.merge(data['full'], data['as'], on='air_store_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284127, 87)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['full_new'] = pd.merge(temp, data['ar_g'], on=['air_store_id', 'visit_date'], how='left')\n",
    "data['full_new'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More features\n",
    "data['full_new']['reserve_ppl_count'] = data['full_new']['reserve_ppl_count_air'] + data['full_new']['reserve_ppl_count_hpg']\n",
    "data['full_new']['reserve_tot_count'] = data['full_new']['reserve_tot_count_air'] + data['full_new']['reserve_tot_count_hpg']\n",
    "data['full_new']['reserve_ppl_mean'] = np.mean(data['full_new'][['reserve_ppl_count_air','reserve_ppl_count_hpg']], axis=1)\n",
    "# data['full_new']['mean_dt_diff_air_hpg'] = data['full_new'][['air_rv_dt_diff','hpg_rv_dt_diff']].apply(lambda x:np.mean(x), axis=1)\n",
    "\n",
    "data['full_new']['lon_plus_lat_air'] = data['full_new']['longitude_air'] + data['full_new']['latitude_air'] \n",
    "\n",
    "data['full_new']['lat_to_mean_lat_air_lv1'] = abs(data['full_new']['latitude_air']-data['full_new']['mean_lat_air_lv1'])\n",
    "data['full_new']['lat_to_max_lat_air_lv1']  = data['full_new']['latitude_air']-data['full_new']['max_lat_air_lv1']\n",
    "data['full_new']['lat_to_min_lat_air_lv1']  = data['full_new']['latitude_air']-data['full_new']['min_lat_air_lv1']\n",
    "data['full_new']['lon_to_mean_lon_air_lv1']  = abs(data['full_new']['longitude_air']-data['full_new']['mean_lon_air_lv1'])\n",
    "data['full_new']['lon_to_max_lon_air_lv1']  = data['full_new']['longitude_air']-data['full_new']['max_lon_air_lv1']\n",
    "data['full_new']['lon_to_min_lon_air_lv1']  = data['full_new']['longitude_air']-data['full_new']['min_lon_air_lv1']\n",
    "data['full_new']['lat_to_mean_lat_air_lv2'] = abs(data['full_new']['latitude_air']-data['full_new']['mean_lat_air_lv2'])\n",
    "data['full_new']['lat_to_max_lat_air_lv2']  = data['full_new']['latitude_air']-data['full_new']['max_lat_air_lv2']\n",
    "data['full_new']['lat_to_min_lat_air_lv2']  = data['full_new']['latitude_air']-data['full_new']['min_lat_air_lv2']\n",
    "data['full_new']['lon_to_mean_lon_air_lv2'] = abs(data['full_new']['longitude_air']-data['full_new']['mean_lon_air_lv2'])\n",
    "data['full_new']['lon_to_max_lon_air_lv2']  = data['full_new']['longitude_air']-data['full_new']['max_lon_air_lv2']\n",
    "data['full_new']['lon_to_min_lon_air_lv2']  = data['full_new']['longitude_air']-data['full_new']['min_lon_air_lv2']\n",
    "\n",
    "data['full_new']['lat_to_mean_lat_hpg_lv1'] = abs(data['full_new']['latitude_hpg']-data['full_new']['mean_lat_hpg_lv1'])\n",
    "data['full_new']['lat_to_max_lat_hpg_lv1']  = data['full_new']['latitude_hpg']-data['full_new']['max_lat_hpg_lv1']\n",
    "data['full_new']['lat_to_min_lat_hpg_lv1']  = data['full_new']['latitude_hpg']-data['full_new']['min_lat_hpg_lv1']\n",
    "data['full_new']['lon_to_mean_lon_hpg_lv1']  = abs(data['full_new']['longitude_hpg']-data['full_new']['mean_lon_hpg_lv1'])\n",
    "data['full_new']['lon_to_max_lon_hpg_lv1']  = data['full_new']['longitude_hpg']-data['full_new']['max_lon_hpg_lv1']\n",
    "data['full_new']['lon_to_min_lon_hpg_lv1']  = data['full_new']['longitude_hpg']-data['full_new']['min_lon_hpg_lv1']\n",
    "data['full_new']['lat_to_mean_lat_hpg_lv2'] = abs(data['full_new']['latitude_hpg']-data['full_new']['mean_lat_hpg_lv2'])\n",
    "data['full_new']['lat_to_max_lat_hpg_lv2']  = data['full_new']['latitude_hpg']-data['full_new']['max_lat_hpg_lv2']\n",
    "data['full_new']['lat_to_min_lat_hpg_lv2']  = data['full_new']['latitude_hpg']-data['full_new']['min_lat_hpg_lv2']\n",
    "data['full_new']['lon_to_mean_lon_hpg_lv2'] = abs(data['full_new']['longitude_hpg']-data['full_new']['mean_lon_hpg_lv2'])\n",
    "data['full_new']['lon_to_max_lon_hpg_lv2']  = data['full_new']['longitude_hpg']-data['full_new']['max_lon_hpg_lv2']\n",
    "data['full_new']['lon_to_min_lon_hpg_lv2']  = data['full_new']['longitude_hpg']-data['full_new']['min_lon_hpg_lv2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "       232258, 252108, 254932, 254932, 254932, 254932, 254932, 254932,\n",
       "       254932, 254932, 254932, 254932, 261572, 261572, 261572, 261572,\n",
       "       261572, 261572, 261572, 261572, 261572, 261572, 261572, 261572,\n",
       "       261572, 261572, 261572, 261572, 261572, 261572, 261572, 261572,\n",
       "       261572, 261572, 261572, 261572, 261572, 261572, 261572, 261572,\n",
       "       261572, 261572, 261572, 261572, 261572, 261572, 261572, 278019,\n",
       "       278019, 278019, 278019, 278019, 278019, 278019, 278019, 278019,\n",
       "       278019, 278019, 278019], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(np.sum(data['full_new'].isnull(), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hol']['visit_date'] = pd.to_datetime(data['hol']['visit_date'])\n",
    "data['hol']['weight1'] = (data['hol'].index + 1) / len(data['hol']) ** 4\n",
    "data['hol']['weight2'] = (data['hol'].index + 1) / len(data['hol']) ** 5\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].dt.date\n",
    "data['hol']['visit_date'] = data['hol']['visit_date'].apply(lambda x: str(x))\n",
    "data['hol'].drop('day_of_week', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_new'] = pd.merge(data['full_new'], data['hol'], on='visit_date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "517"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = data['full_new'].visit_date.unique()\n",
    "b2 = data['hol'].visit_date.unique()\n",
    "len(set(b1).intersection(b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>dow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>doy</th>\n",
       "      <th>dom</th>\n",
       "      <th>woy</th>\n",
       "      <th>...</th>\n",
       "      <th>lon_to_min_lon_hpg_lv1</th>\n",
       "      <th>lat_to_mean_lat_hpg_lv2</th>\n",
       "      <th>lat_to_max_lat_hpg_lv2</th>\n",
       "      <th>lat_to_min_lat_hpg_lv2</th>\n",
       "      <th>lon_to_mean_lon_hpg_lv2</th>\n",
       "      <th>lon_to_max_lon_hpg_lv2</th>\n",
       "      <th>lon_to_min_lon_hpg_lv2</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>weight1</th>\n",
       "      <th>weight2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.819622e-10</td>\n",
       "      <td>3.519578e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.959593e-10</td>\n",
       "      <td>3.790315e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.099564e-10</td>\n",
       "      <td>4.061052e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.239535e-10</td>\n",
       "      <td>4.331789e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.519477e-10</td>\n",
       "      <td>4.873262e-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id   id  visit_date  visitors  dow  year  month  doy  \\\n",
       "0  air_ba937bf13d40fb24  NaN  2016-01-13        25    2  2016      1   13   \n",
       "1  air_ba937bf13d40fb24  NaN  2016-01-14        32    3  2016      1   14   \n",
       "2  air_ba937bf13d40fb24  NaN  2016-01-15        29    4  2016      1   15   \n",
       "3  air_ba937bf13d40fb24  NaN  2016-01-16        22    5  2016      1   16   \n",
       "4  air_ba937bf13d40fb24  NaN  2016-01-18         6    0  2016      1   18   \n",
       "\n",
       "   dom  woy      ...       lon_to_min_lon_hpg_lv1  lat_to_mean_lat_hpg_lv2  \\\n",
       "0   31    2      ...                          NaN                      NaN   \n",
       "1   31    2      ...                          NaN                      NaN   \n",
       "2   31    2      ...                          NaN                      NaN   \n",
       "3   31    2      ...                          NaN                      NaN   \n",
       "4   31    3      ...                          NaN                      NaN   \n",
       "\n",
       "   lat_to_max_lat_hpg_lv2 lat_to_min_lat_hpg_lv2  lon_to_mean_lon_hpg_lv2  \\\n",
       "0                     NaN                    NaN                      NaN   \n",
       "1                     NaN                    NaN                      NaN   \n",
       "2                     NaN                    NaN                      NaN   \n",
       "3                     NaN                    NaN                      NaN   \n",
       "4                     NaN                    NaN                      NaN   \n",
       "\n",
       "   lon_to_max_lon_hpg_lv2  lon_to_min_lon_hpg_lv2  holiday_flg       weight1  \\\n",
       "0                     NaN                     NaN            0  1.819622e-10   \n",
       "1                     NaN                     NaN            0  1.959593e-10   \n",
       "2                     NaN                     NaN            0  2.099564e-10   \n",
       "3                     NaN                     NaN            0  2.239535e-10   \n",
       "4                     NaN                     NaN            0  2.519477e-10   \n",
       "\n",
       "        weight2  \n",
       "0  3.519578e-13  \n",
       "1  3.790315e-13  \n",
       "2  4.061052e-13  \n",
       "3  4.331789e-13  \n",
       "4  4.873262e-13  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['full_new'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cat to cat\n",
    "data['full_new']['air_area_genre'] = data['full_new']['air_area_name'].astype(str) + '_' + data['full_new']['air_genre_name'].astype(str)\n",
    "\n",
    "cat_cat_vars = ['air_area_genre']\n",
    "\n",
    "lb = LabelEncoder()\n",
    "for cat_var in cat_cat_vars:\n",
    "    data['full_new'][cat_var] = lb.fit_transform(data['full_new'][cat_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target aggregation\n",
    "tmp = data['full_new'][:train_size].groupby(['air_store_id','dow','holiday_flg'])['visitors'].\\\n",
    "        agg([np.mean, np.max, np.min, np.median]).\\\n",
    "        reset_index().\\\n",
    "        rename(columns={'mean':'mean_visitors',\n",
    "                       'amax':'max_visitors',\n",
    "                       'amin':'min_visitors',\n",
    "                       'median':'median_visitors'})\n",
    "data['full_new'] = pd.merge(data['full_new'], tmp, how='left', on=['air_store_id','dow','holiday_flg'])\n",
    "\n",
    "tmp = data['full_new'][:train_size].groupby(['air_store_id','dow', 'holiday_flg']).\\\n",
    "            apply(lambda x:( (x.weight2 * x.visitors).sum() / x.weight2.sum() )).\\\n",
    "            reset_index().rename(columns={0:'wmean_visitors'})\n",
    "        \n",
    "data['full_new'] = pd.merge(data['full_new'], tmp, how='left', on=['air_store_id','dow','holiday_flg'])  \n",
    "\n",
    "target_aggr_vars = ['mean_visitors', 'max_visitors', 'min_visitors', 'median_visitors', 'wmean_visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>air_store_id</th>\n",
       "      <th>id</th>\n",
       "      <th>visit_date</th>\n",
       "      <th>visitors</th>\n",
       "      <th>dow</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>doy</th>\n",
       "      <th>dom</th>\n",
       "      <th>woy</th>\n",
       "      <th>...</th>\n",
       "      <th>lon_to_min_lon_hpg_lv2</th>\n",
       "      <th>holiday_flg</th>\n",
       "      <th>weight1</th>\n",
       "      <th>weight2</th>\n",
       "      <th>air_area_genre</th>\n",
       "      <th>mean_visitors</th>\n",
       "      <th>max_visitors</th>\n",
       "      <th>min_visitors</th>\n",
       "      <th>median_visitors</th>\n",
       "      <th>wmean_visitors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-13</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.819622e-10</td>\n",
       "      <td>3.519578e-13</td>\n",
       "      <td>145</td>\n",
       "      <td>24.095238</td>\n",
       "      <td>57.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>22.601896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-14</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1.959593e-10</td>\n",
       "      <td>3.790315e-13</td>\n",
       "      <td>145</td>\n",
       "      <td>20.450000</td>\n",
       "      <td>45.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>18.204524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-15</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.099564e-10</td>\n",
       "      <td>4.061052e-13</td>\n",
       "      <td>145</td>\n",
       "      <td>35.218750</td>\n",
       "      <td>61.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>35.5</td>\n",
       "      <td>34.518831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-16</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.239535e-10</td>\n",
       "      <td>4.331789e-13</td>\n",
       "      <td>145</td>\n",
       "      <td>27.828125</td>\n",
       "      <td>53.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>27.950415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>air_ba937bf13d40fb24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2016-01-18</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2.519477e-10</td>\n",
       "      <td>4.873262e-13</td>\n",
       "      <td>145</td>\n",
       "      <td>13.754386</td>\n",
       "      <td>34.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.381561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 124 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           air_store_id   id  visit_date  visitors  dow  year  month  doy  \\\n",
       "0  air_ba937bf13d40fb24  NaN  2016-01-13        25    2  2016      1   13   \n",
       "1  air_ba937bf13d40fb24  NaN  2016-01-14        32    3  2016      1   14   \n",
       "2  air_ba937bf13d40fb24  NaN  2016-01-15        29    4  2016      1   15   \n",
       "3  air_ba937bf13d40fb24  NaN  2016-01-16        22    5  2016      1   16   \n",
       "4  air_ba937bf13d40fb24  NaN  2016-01-18         6    0  2016      1   18   \n",
       "\n",
       "   dom  woy       ...        lon_to_min_lon_hpg_lv2  holiday_flg  \\\n",
       "0   31    2       ...                           NaN            0   \n",
       "1   31    2       ...                           NaN            0   \n",
       "2   31    2       ...                           NaN            0   \n",
       "3   31    2       ...                           NaN            0   \n",
       "4   31    3       ...                           NaN            0   \n",
       "\n",
       "        weight1       weight2  air_area_genre  mean_visitors  max_visitors  \\\n",
       "0  1.819622e-10  3.519578e-13             145      24.095238          57.0   \n",
       "1  1.959593e-10  3.790315e-13             145      20.450000          45.0   \n",
       "2  2.099564e-10  4.061052e-13             145      35.218750          61.0   \n",
       "3  2.239535e-10  4.331789e-13             145      27.828125          53.0   \n",
       "4  2.519477e-10  4.873262e-13             145      13.754386          34.0   \n",
       "\n",
       "   min_visitors  median_visitors  wmean_visitors  \n",
       "0           7.0             25.0       22.601896  \n",
       "1           7.0             21.0       18.204524  \n",
       "2          17.0             35.5       34.518831  \n",
       "3           6.0             27.0       27.950415  \n",
       "4           2.0             12.0       12.381561  \n",
       "\n",
       "[5 rows x 124 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['full_new'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "visit_num_vars = ['dow', 'year', 'month', 'doy', 'dom', 'woy', 'is_month_end', 'date_int']\n",
    "\n",
    "reserve_num_vars = ['reserve_ppl_count_air', 'reserve_tot_count_air', 'avg_reserve_hr_day_air',\n",
    "\t'max_reserve_hr_air', 'min_reserve_hr_air', 'mean_reserve_hr_air', 'min_reserve_dy_air',\n",
    "    'mean_reserve_dy_air', 'max_reserve_dy_air', 'reserve_ppl_count_hpg', 'reserve_tot_count_hpg',\n",
    "    'avg_reserve_hr_day_hpg', 'max_reserve_hr_hpg', 'min_reserve_hr_hpg', 'mean_reserve_hr_hpg',\n",
    "     'min_reserve_dy_hpg', 'mean_reserve_dy_hpg', 'max_reserve_dy_hpg']\n",
    "\n",
    "store_num_vars = [\n",
    "    'latitude_air', 'longitude_air', 'air_stores_on_same_addr', 'air_stores_lv1',\n",
    "    'air_stores_lv2', 'air_stores_lv3', 'mean_lat_air_lv1', 'max_lat_air_lv1',\n",
    "    'min_lat_air_lv1', 'mean_lon_air_lv1', 'max_lon_air_lv1', 'min_lon_air_lv1',\n",
    "    'mean_lat_air_lv2',  'max_lat_air_lv2', 'min_lat_air_lv2', 'mean_lon_air_lv2',\n",
    "    'max_lon_air_lv2',   'min_lon_air_lv2', 'air_genre_count', 'air_genre_count_lv1',\n",
    "    'air_genre_count_lv2',   'air_genre_count_lv3', 'latitude_hpg', 'longitude_hpg',\n",
    "    'hpg_stores_on_same_addr', 'hpg_stores_lv1', 'hpg_stores_lv2', 'hpg_stores_lv3',\n",
    "    'mean_lat_hpg_lv1', 'max_lat_hpg_lv1', 'min_lat_hpg_lv1', 'mean_lon_hpg_lv1',\n",
    "    'max_lon_hpg_lv1', 'min_lon_hpg_lv1', 'mean_lat_hpg_lv2', 'max_lat_hpg_lv2',\n",
    "    'min_lat_hpg_lv2', 'mean_lon_hpg_lv2', 'max_lon_hpg_lv2', 'min_lon_hpg_lv2',\n",
    "    'hpg_genre_count', 'hpg_genre_count_lv1', 'hpg_genre_count_lv2', 'hpg_genre_count_lv3'\n",
    "    ]\n",
    "store_cat_vars = [\n",
    "    'air_genre_name', 'air_lv1', 'air_lv2', 'air_lv3', 'air_lv4',\n",
    "    'hpg_genre_name', 'hpg_lv1', 'hpg_lv2', 'hpg_lv3'\n",
    "]\n",
    "\n",
    "interacts_vars = [\n",
    "    'reserve_ppl_count', 'reserve_tot_count', 'reserve_ppl_mean', 'lon_plus_lat_air',\n",
    "    'lat_to_mean_lat_air_lv1', 'lat_to_max_lat_air_lv1', 'lat_to_min_lat_air_lv1',\n",
    "    'lon_to_mean_lon_air_lv1', 'lon_to_max_lon_air_lv1', 'lon_to_min_lon_air_lv1',\n",
    "    'lat_to_mean_lat_air_lv2', 'lat_to_max_lat_air_lv2', 'lat_to_min_lat_air_lv2',\n",
    "    'lon_to_mean_lon_air_lv2', 'lon_to_max_lon_air_lv2', 'lon_to_min_lon_air_lv2',\n",
    "    'lat_to_mean_lat_hpg_lv1', 'lat_to_max_lat_hpg_lv1', 'lat_to_min_lat_hpg_lv1',\n",
    "    'lon_to_mean_lon_hpg_lv1', 'lon_to_max_lon_hpg_lv1', 'lon_to_min_lon_hpg_lv1',\n",
    "    'lat_to_mean_lat_hpg_lv2', 'lat_to_max_lat_hpg_lv2', 'lat_to_min_lat_hpg_lv2',\n",
    "    'lon_to_mean_lon_hpg_lv2', 'lon_to_max_lon_hpg_lv2', 'lon_to_min_lon_hpg_lv2',\t\n",
    "]\n",
    "\n",
    "hol_mix_vars = ['weight1', 'weight2', 'holiday_flg']\n",
    "\n",
    "target_agg_vars = ['mean_visitors', 'max_visitors', 'min_visitors', 'median_visitors', 'wmean_visitors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: (252108, 124) test data size: (32019, 124)\n"
     ]
    }
   ],
   "source": [
    "TARGET = 'visitors'\n",
    "data['full_new'].visit_date = pd.to_datetime(data['full_new'].visit_date)\n",
    "train = data['full_new'][:train_size]\n",
    "test = data['full_new'][train_size:]\n",
    "y = data['full_new'][:train_size][TARGET][:train_size].values\n",
    "IDs = data['full_new'][train_size:][train_size:].id.values\n",
    "print ('train data size:', train.shape, 'test data size:', test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197019, 115) (220517, 115) 197019 228610\n"
     ]
    }
   ],
   "source": [
    "train_x = train[(train['visit_date']<=pd.to_datetime('2017-03-09')) \n",
    "      & (train['visit_date']>pd.to_datetime('2016-04-01'))][features].values\n",
    "train_y = np.log1p(train[((train['visit_date']<=pd.to_datetime('2017-03-09')) \n",
    "      & (train['visit_date']>pd.to_datetime('2016-04-01')))]['visitors'].values)\n",
    "\n",
    "watch_x = train[(train['visit_date']<=pd.to_datetime('2017-03-09'))][features].values\n",
    "watch_y = np.log1p(train[(train['visit_date']> pd.to_datetime('2016-04-01'))]['visitors'].values)\n",
    "print (train_x.shape, watch_x.shape, train_y.shape[0], watch_y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2017, 3, 9)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "watch=xgb.DMatrix(watch_x,label=watch_x)\n",
    "\n",
    "watchlist  = [ (xgtrain,'train'),(watch,'eval')]\n",
    "\n",
    "\n",
    "best_xgb_params = {\n",
    "    'colsample_bytree': 0.7,\n",
    "    'eta': 0.1,\n",
    "    'gamma': 1,\n",
    "    'max_depth': 10,\n",
    "    'min_child_weight': 3,\n",
    "    'nthread': 8,\n",
    "    'objective': 'reg:linear',\n",
    "    'seed': 1234,\n",
    "    'subsample': 1\n",
    "}\n",
    "\n",
    "print (best_xgb_params)\n",
    "\n",
    "model = xgb.train(best_xgb_params, \n",
    "                  xgtrain, \n",
    "                  num_boost_round=100000,\n",
    "                  evals=watchlist,\n",
    "                  early_stopping_rounds=50,\n",
    "                  verbose_eval=50)    \n",
    "best_iteration = model.best_iteration\n",
    "best_score = model.best_score\n",
    "print ('best_score: %f, best_iteration: %d' % (best_score, best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
